{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-23T22:48:14.425690Z",
     "iopub.status.busy": "2022-01-23T22:48:14.425337Z",
     "iopub.status.idle": "2022-01-23T22:48:14.431040Z",
     "shell.execute_reply": "2022-01-23T22:48:14.430171Z",
     "shell.execute_reply.started": "2022-01-23T22:48:14.425654Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 999)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-23T22:20:09.105352Z",
     "iopub.status.busy": "2022-01-23T22:20:09.105046Z",
     "iopub.status.idle": "2022-01-23T22:20:26.780305Z",
     "shell.execute_reply": "2022-01-23T22:20:26.779164Z",
     "shell.execute_reply.started": "2022-01-23T22:20:09.105321Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "# !pip install pystan==2.19.1.1\n",
    "# !pip install prophet\n",
    "\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-23T22:20:26.782386Z",
     "iopub.status.busy": "2022-01-23T22:20:26.782037Z",
     "iopub.status.idle": "2022-01-23T22:20:26.856239Z",
     "shell.execute_reply": "2022-01-23T22:20:26.855316Z",
     "shell.execute_reply.started": "2022-01-23T22:20:26.782348Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/tabular-playground-series-jan-2022/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/tabular-playground-series-jan-2022/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(train\u001b[38;5;241m.\u001b[39mdate)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(train\u001b[38;5;241m.\u001b[39minfo())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tps/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tps/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tps/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tps/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tps/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tps/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/tabular-playground-series-jan-2022/train.csv'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\n",
    "train['date'] = pd.to_datetime(train.date)\n",
    "print(train.info())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-23T22:20:26.858496Z",
     "iopub.status.busy": "2022-01-23T22:20:26.858164Z",
     "iopub.status.idle": "2022-01-23T22:20:26.900059Z",
     "shell.execute_reply": "2022-01-23T22:20:26.899301Z",
     "shell.execute_reply.started": "2022-01-23T22:20:26.858451Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\n",
    "test['date'] = pd.to_datetime(test.date)\n",
    "print(test.info())\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Metric\n",
    "https://www.kaggle.com/cpmpml/smape-weirdness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T22:20:26.902326Z",
     "iopub.status.busy": "2022-01-23T22:20:26.902036Z",
     "iopub.status.idle": "2022-01-23T22:20:26.908304Z",
     "shell.execute_reply": "2022-01-23T22:20:26.907518Z",
     "shell.execute_reply.started": "2022-01-23T22:20:26.902297Z"
    }
   },
   "outputs": [],
   "source": [
    "def SMAPE(y_true, y_pred):\n",
    "    denominator = (y_true + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T22:20:26.910249Z",
     "iopub.status.busy": "2022-01-23T22:20:26.909599Z",
     "iopub.status.idle": "2022-01-23T22:20:31.198362Z",
     "shell.execute_reply": "2022-01-23T22:20:31.197491Z",
     "shell.execute_reply.started": "2022-01-23T22:20:26.910204Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.relplot(data=train, x='date', y='num_sold', row='country', col='store', hue='product',\n",
    "            aspect=3, height=2.5, kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T22:20:31.199908Z",
     "iopub.status.busy": "2022-01-23T22:20:31.199666Z",
     "iopub.status.idle": "2022-01-23T22:20:31.218718Z",
     "shell.execute_reply": "2022-01-23T22:20:31.217818Z",
     "shell.execute_reply.started": "2022-01-23T22:20:31.199877Z"
    }
   },
   "outputs": [],
   "source": [
    "val = train[train.date >= '2018-01-01'].copy()\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "train = train[train.date < '2018-01-01'].copy()\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train.rename({'date':'ds', 'num_sold':'y'}, axis=1, inplace=True)\n",
    "val.rename({'date':'ds', 'num_sold':'y'}, axis=1, inplace=True)\n",
    "\n",
    "print('Train', train.shape, '| Start', train.ds.min(), '| End', train.ds.max())\n",
    "print('Val', val.shape, '| Start', val.ds.min(), '| End', val.ds.max())\n",
    "print('Test', test.shape, '| Start', test.ds.min(), '| End', test.ds.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T22:35:58.851126Z",
     "iopub.status.busy": "2022-01-23T22:35:58.850574Z",
     "iopub.status.idle": "2022-01-23T22:38:20.493959Z",
     "shell.execute_reply": "2022-01-23T22:38:20.493143Z",
     "shell.execute_reply.started": "2022-01-23T22:35:58.851062Z"
    }
   },
   "outputs": [],
   "source": [
    "for country in train_.country.unique():\n",
    "    for store in train_.store.unique():\n",
    "        for product in train_['product'].unique():    \n",
    "            print(country, store, product)\n",
    "            \n",
    "            # Subsets for current country, stores and product\n",
    "            train_idx = train[(train.country==country) & \n",
    "                              (train.store==store) &\n",
    "                              (train['product']==product)].index\n",
    "            \n",
    "            train_sub = train.loc[train_idx].copy()\n",
    "            \n",
    "            val_idx = val[(val.country==country) & \n",
    "                          (val.store==store) &\n",
    "                          (val['product']==product)].index\n",
    "            \n",
    "            val_sub = val.loc[val_idx].copy()\n",
    "            \n",
    "            # Define the model and fit it on the train subset of data\n",
    "            model = Prophet()\n",
    "            model.fit(train_sub)\n",
    "            \n",
    "            # Predict for train e validation datasets\n",
    "            train_preds = model.predict(train_sub)\n",
    "            val_preds = model.predict(val_sub)\n",
    "            \n",
    "            # Calculate scores base on comp metric SMAPE\n",
    "            train_score = SMAPE(train_sub.y.values, train_preds.yhat.values)\n",
    "            val_score = SMAPE(val_sub.y.values, val_preds.yhat.values)\n",
    "            \n",
    "            print()\n",
    "            print('--------------------------------------------------------------------------')\n",
    "            print('Train Score', country, store, product, 'SMAPE: {:f}'.format(train_score))\n",
    "            print('Val Score', country, store, product, 'SMAPE: {:f}'.format(val_score))\n",
    "            print('--------------------------------------------------------------------------')\n",
    "            print()            \n",
    "            \n",
    "            # Add predictions to train and validation datasets\n",
    "            train.loc[train_idx, 'yhat'] = train_preds.yhat.values\n",
    "            val.loc[val_idx, 'yhat'] = val_preds.yhat.values\n",
    "\n",
    "print()\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Train Score', 'SMAPE: {:f}'.format(SMAPE(train.y.values, train.yhat.values)))\n",
    "print('Val Score', 'SMAPE: {:f}'.format(SMAPE(val.y.values, val.yhat.values)))\n",
    "print('--------------------------------------------------------------------------')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Holidays\n",
    "https://www.kaggle.com/gunesevitan/tabular-playground-series-jan-2022-prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T22:38:57.107686Z",
     "iopub.status.busy": "2022-01-23T22:38:57.107094Z",
     "iopub.status.idle": "2022-01-23T22:38:57.129228Z",
     "shell.execute_reply": "2022-01-23T22:38:57.128382Z",
     "shell.execute_reply.started": "2022-01-23T22:38:57.107640Z"
    }
   },
   "outputs": [],
   "source": [
    "new_year = pd.DataFrame({\n",
    "  'holiday': 'new_year',\n",
    "  'ds': pd.to_datetime(['2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01', '2019-01-01']),\n",
    "  'lower_window': -1,\n",
    "  'upper_window': 0,\n",
    "})\n",
    "\n",
    "easter = pd.DataFrame({\n",
    "  'holiday': 'easter',\n",
    "  'ds': pd.to_datetime(['2015-04-05', '2016-03-27', '2017-04-16', '2018-04-01', '2019-04-21']),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 7,\n",
    "})\n",
    "\n",
    "holidays = pd.concat((new_year, easter))\n",
    "holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T22:38:58.231813Z",
     "iopub.status.busy": "2022-01-23T22:38:58.231517Z",
     "iopub.status.idle": "2022-01-23T22:41:21.737103Z",
     "shell.execute_reply": "2022-01-23T22:41:21.736102Z",
     "shell.execute_reply.started": "2022-01-23T22:38:58.231784Z"
    }
   },
   "outputs": [],
   "source": [
    "for country in train_.country.unique():\n",
    "    for store in train_.store.unique():\n",
    "        for product in train_['product'].unique():    \n",
    "            print(country, store, product)\n",
    "            \n",
    "            # Subsets for current country, stores and product\n",
    "            train_idx = train[(train.country==country) & \n",
    "                              (train.store==store) &\n",
    "                              (train['product']==product)].index\n",
    "            \n",
    "            train_sub = train.loc[train_idx].copy()\n",
    "            \n",
    "            val_idx = val[(val.country==country) & \n",
    "                          (val.store==store) &\n",
    "                          (val['product']==product)].index\n",
    "            \n",
    "            val_sub = val.loc[val_idx].copy()\n",
    "            \n",
    "            # Define the model and fit it on the train subset of data\n",
    "            model = Prophet(holidays=holidays)\n",
    "            model.fit(train_sub)\n",
    "            \n",
    "            # Predict for train e validation datasets\n",
    "            train_preds = model.predict(train_sub)\n",
    "            val_preds = model.predict(val_sub)\n",
    "            \n",
    "            # Calculate scores base on comp metric SMAPE\n",
    "            train_score = SMAPE(train_sub.y.values, train_preds.yhat.values)\n",
    "            val_score = SMAPE(val_sub.y.values, val_preds.yhat.values)\n",
    "            \n",
    "            print()\n",
    "            print('--------------------------------------------------------------------------')\n",
    "            print('Train Score', country, store, product, 'SMAPE: {:f}'.format(train_score))\n",
    "            print('Val Score', country, store, product, 'SMAPE: {:f}'.format(val_score))\n",
    "            print('--------------------------------------------------------------------------')\n",
    "            print()            \n",
    "            \n",
    "            # Add predictions to train and validation datasets\n",
    "            train.loc[train_idx, 'yhat'] = train_preds.yhat.values\n",
    "            val.loc[val_idx, 'yhat'] = val_preds.yhat.values\n",
    "\n",
    "print()\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Train Score', 'SMAPE: {:f}'.format(SMAPE(train.y.values, train.yhat.values)))\n",
    "print('Val Score', 'SMAPE: {:f}'.format(SMAPE(val.y.values, val.yhat.values)))\n",
    "print('--------------------------------------------------------------------------')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Parameters\n",
    "https://www.kaggle.com/gunesevitan/tabular-playground-series-jan-2022-prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T23:22:51.334886Z",
     "iopub.status.busy": "2022-01-23T23:22:51.334352Z",
     "iopub.status.idle": "2022-01-23T23:23:27.200271Z",
     "shell.execute_reply": "2022-01-23T23:23:27.198817Z",
     "shell.execute_reply.started": "2022-01-23T23:22:51.334838Z"
    }
   },
   "outputs": [],
   "source": [
    "for country in train_.country.unique():\n",
    "    for store in train_.store.unique():\n",
    "        for product in train_['product'].unique():    \n",
    "            print(country, store, product)\n",
    "            \n",
    "            # Subsets for current country, stores and product\n",
    "            train_idx = train[(train.country==country) & \n",
    "                              (train.store==store) &\n",
    "                              (train['product']==product)].index\n",
    "            \n",
    "            train_sub = train.loc[train_idx].copy()\n",
    "            \n",
    "            val_idx = val[(val.country==country) & \n",
    "                          (val.store==store) &\n",
    "                          (val['product']==product)].index\n",
    "            \n",
    "            val_sub = val.loc[val_idx].copy()\n",
    "            \n",
    "            # Define the model and fit it on the train subset of data\n",
    "            model = Prophet(\n",
    "                growth='linear',\n",
    "                holidays=holidays,\n",
    "                n_changepoints=10,\n",
    "                changepoint_range=0.4,\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=True,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='additive',\n",
    "                seasonality_prior_scale=25,\n",
    "                holidays_prior_scale=100,\n",
    "                changepoint_prior_scale=0.01,\n",
    "                interval_width=0.5,\n",
    "                uncertainty_samples=False\n",
    "            )\n",
    "            model.fit(train_sub)\n",
    "            \n",
    "            # Predict for train e validation datasets\n",
    "            train_preds = model.predict(train_sub)\n",
    "            val_preds = model.predict(val_sub)\n",
    "            \n",
    "            # Calculate scores base on comp metric SMAPE\n",
    "            train_score = SMAPE(train_sub.y.values, train_preds.yhat.values)\n",
    "            val_score = SMAPE(val_sub.y.values, val_preds.yhat.values)\n",
    "            \n",
    "            print()\n",
    "            print('--------------------------------------------------------------------------')\n",
    "            print('Train Score', country, store, product, 'SMAPE: {:f}'.format(train_score))\n",
    "            print('Val Score', country, store, product, 'SMAPE: {:f}'.format(val_score))\n",
    "            print('--------------------------------------------------------------------------')\n",
    "            print()            \n",
    "            \n",
    "            # Add predictions to train and validation datasets\n",
    "            train.loc[train_idx, 'yhat'] = train_preds.yhat.values\n",
    "            val.loc[val_idx, 'yhat'] = val_preds.yhat.values\n",
    "\n",
    "print()\n",
    "print('--------------------------------------------------------------------------')\n",
    "print('Train Score', 'SMAPE: {:f}'.format(SMAPE(train.y.values, train.yhat.values)))\n",
    "print('Val Score', 'SMAPE: {:f}'.format(SMAPE(val.y.values, val.yhat.values)))\n",
    "print('--------------------------------------------------------------------------')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# growth='linear',\n",
    "# holidays=holidays,\n",
    "# n_changepoints=10,\n",
    "# changepoint_range=0.4,\n",
    "# yearly_seasonality=True,\n",
    "# weekly_seasonality=True,\n",
    "# daily_seasonality=False,\n",
    "# seasonality_mode='additive',\n",
    "# seasonality_prior_scale=25,\n",
    "# holidays_prior_scale=100,\n",
    "# changepoint_prior_scale=0.01,\n",
    "# interval_width=0.5,\n",
    "# uncertainty_samples=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T23:53:57.526146Z",
     "iopub.status.busy": "2022-01-23T23:53:57.525835Z",
     "iopub.status.idle": "2022-01-23T23:53:57.560305Z",
     "shell.execute_reply": "2022-01-23T23:53:57.559233Z",
     "shell.execute_reply.started": "2022-01-23T23:53:57.526109Z"
    }
   },
   "outputs": [],
   "source": [
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "\n",
    "all_data = pd.concat([train, val], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "df = all_data[(all_data.country=='Finland') & \n",
    "              (all_data.store=='KaggleMart') &\n",
    "              (all_data['product']=='Kaggle Mug')].copy()\n",
    "\n",
    "param_grid = {  \n",
    "    'changepoint_prior_scale': [0.001, 0.01, 0.1],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1, 10, 25],\n",
    "    'holidays_prior_scale':[0.01, 0.1, 1, 10],\n",
    "    'changepoint_range':[0.7, 0.8, 0.9],\n",
    "    'holidays':[holidays]\n",
    "}\n",
    "\n",
    "cutoffs = pd.to_datetime(['2015-12-31', '2016-12-31', '2017-12-31'])\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "print(len(all_params))\n",
    "smapes = []  # Store the RMSEs for each params here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T23:54:06.326786Z",
     "iopub.status.busy": "2022-01-23T23:54:06.326145Z",
     "iopub.status.idle": "2022-01-23T23:54:25.685976Z",
     "shell.execute_reply": "2022-01-23T23:54:25.682114Z",
     "shell.execute_reply.started": "2022-01-23T23:54:06.326741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use cross validation to evaluate all parameters\n",
    "for params in all_params:\n",
    "    m = Prophet(**params).fit(df)  # Fit model with given params\n",
    "    df_cv = cross_validation(m, initial=1095, cutoffs=cutoffs, horizon='365 days', parallel=\"processes\")\n",
    "    df_p = performance_metrics(df_cv, rolling_window=1)\n",
    "    smapes.append(df_p['smape'].values[0])\n",
    "\n",
    "# Find the best parameters\n",
    "tuning_results = pd.DataFrame(all_params)\n",
    "tuning_results['smape'] = smapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-23T23:46:15.838469Z",
     "iopub.status.idle": "2022-01-23T23:46:15.839229Z",
     "shell.execute_reply": "2022-01-23T23:46:15.838933Z",
     "shell.execute_reply.started": "2022-01-23T23:46:15.838898Z"
    }
   },
   "outputs": [],
   "source": [
    "tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T23:07:24.399806Z",
     "iopub.status.busy": "2022-01-23T23:07:24.399010Z",
     "iopub.status.idle": "2022-01-23T23:07:24.408548Z",
     "shell.execute_reply": "2022-01-23T23:07:24.407895Z",
     "shell.execute_reply.started": "2022-01-23T23:07:24.399763Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cv[df_cv.cutoff=='2017-12-31T00:00:00.000000000'].ds.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T20:34:30.259452Z",
     "iopub.status.busy": "2022-01-23T20:34:30.259187Z",
     "iopub.status.idle": "2022-01-23T20:34:30.281164Z",
     "shell.execute_reply": "2022-01-23T20:34:30.280239Z",
     "shell.execute_reply.started": "2022-01-23T20:34:30.259424Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = test[['row_id']]\n",
    "submission = pd.concat([submission, test_preds[['yhat']]], axis=0)\n",
    "submission.rename({'yhat':'num_sold'}, axis=1, inplace=True)\n",
    "# submission['num_sold'] = np.ceil(submission.num_sold)\n",
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T20:31:07.627806Z",
     "iopub.status.busy": "2022-01-23T20:31:07.627068Z",
     "iopub.status.idle": "2022-01-23T20:31:07.674736Z",
     "shell.execute_reply": "2022-01-23T20:31:07.673963Z",
     "shell.execute_reply.started": "2022-01-23T20:31:07.627757Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tps",
   "language": "python",
   "name": "tps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
